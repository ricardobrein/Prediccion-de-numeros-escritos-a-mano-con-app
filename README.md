# Un Predictor de n칰meros escritos a mano utilizando el cl치sico dataset MNIST.
### Viene incluida una peque침a interfaz con tkinter para probar el modelo escribiendo los n칰mero directamente con el mouse.

![Predictor con interfaz para dibujar](media/gif_pred.gif)

La identificaci칩n de n칰meros escritos a mano mediante redes neuronales es una tarea cl치sica en Deep Learning, sirve para tareas de visi칩n por computadora. Utilizando conjuntos de datos etiquetados, **como el conjunto de datos MNIST**, que contiene miles de im치genes de n칰meros y sus etiquetas reales. Estos modelos aprenden a reconocer y clasificar d칤gitos num칠ricos.

Este tipo tareas tienen aplicaciones practicas en procesamiento de im치genes, digitalizaci칩n de documentos, clasificaci칩n autom치tica de formularios y detecci칩n de fraudes.

**Como he mencionado, vamos a entrenar un modelo de red neuronal utilizando el conjunto de datos [MNIST](https://datascience.eu/es/procesamiento-del-lenguaje-natural/base-de-datos-del-mnist/#:~:text=la%20base%20de%20datos%20del,sistemas%20de%20manejo%20de%20im%C3%A1genes.)**
<img src="media/mnist2.png" alt="Numero 5 de MNIST" style="width:700px;">


### Voy a intentar explicar cada uno de los pasos de manera sencilla y tambien puedes ver el c칩digo en el repositorio.

El dataset MNIST contiene las im치genes de entrenamiento y prueba, as칤 como las etiquetas correspondientes que indican qu칠 d칤gito representa cada imagen.

    import tensorflow as tf
    from tensorflow.keras.datasets import mnist
    import numpy as np
  
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

 **Preprocesamiento de los datos**

     x_train = x_train / 255.0 
     x_test = x_test / 255.0

Las imagenes de MNIST est치n en escala de grises. Primero se realiza un preprocesamiento de los datos dividiendo los valores de cada pixel entre 255 para normalizarlos y que est칠n en un rango de 0 a 1. La escala de grises generalmente se representa en n칰meros de 0 a 255.

**Esto facilita el paso por la red neuronal.**

> En este rango, el valor 0 representa el **negro absoluto (sin intensidad de luz)** y el valor 255 representa el **blanco absoluto** (m치xima intensidad de luz).
>> La raz칩n por la cual se utiliza el rango de 0 a 255 es por la representaci칩n de 8 bits, donde cada p칤xel en una imagen _en blanco y negro_, se almacena como un valor de 8 bits (1 byte). Con 8 bits, se pueden representar 2^8 = 256 valores distintos, es decir, desde 0 hasta 255. Cada valor representa un nivel de intensidad de luz en la escala de grises.

### Paso 3: Crear el modelo de la red neuronal

    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Flatten, Dense
    
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28)))
    model.add(Dense(units=128, activation='relu'))
    model.add(Dense(units=128, activation='relu'))
    model.add(Dense(units=128, activation='relu'))
    model.add(Dense(units=10, activation='softmax'))
    
1. Creamos el modelo de red neuronal utilizando la API de Keras. El modelo consta de una capa **Flatten()** o de _aplanamiento_ que convierte las caracter칤sticas 2D en un vector 1D. 
2. Seguimos con tres capas ocultas Dense(units=128, activation='relu'): las capas Dense son capas completamente conectadas con **128 unidades (neuronas)** y funci칩n de activaci칩n ReLU(_explicaci칩n m치s abajo_). Debo decir que esto puede variar y podriamos probar por ejemplo con una sola capa totalmente conectada pero de 1024 unidades.
3. Por ultimo una capa de salida **fully connected** (Dense) pero con 10 unidades (una para cada d칤gito posible) y funci칩n de activaci칩n softmax. La funci칩n softmax asigna probabilidades a cada clase por eso se utiliza en problemas de clasificaci칩n multiclase.

### Paso 4: Compilar el modelo.

    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
              
B치sicamente, compilar es definir el modelo y sus hiperpar치metros, especificando el optimizador, la funci칩n de p칠rdida y las m칠tricas que se utilizar치n durante el entrenamiento. En este caso, se utiliza el optimizador [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), la funci칩n de p칠rdida **sparse_categorical_crossentropy** es apropiada para clasificaci칩n multiclase adem치s medimos la precisi칩n.
![Funcion de perdida](media/loss.png)

### Paso 5: Entrenar el modelo.
Se entrena el modelo utilizando los datos de entrenamiento durante 50 epochs. Durante el entrenamiento, el modelo aprender치 a reconocer y clasificar los d칤gitos de el dataset MNIST

### Paso 6: Evaluar el modelo con el conjunto de prueba.

![Grafica de perdida y precision](media/grafica.png)

Una vez entrenado el modelo, se eval칰a su rendimiento utilizando el conjunto de prueba. Se calcula la p칠rdida y la precisi칩n mientras ve칤a las im치genes de prueba. La precisi칩n indica qu칠 tan bien clasifica el modelo los d칤gitos que nunca ha visto.

## Creaci칩n de la app de Tkinter para dibujar el n칰mero

Si queremos probar la app con ejemplos reales, que mejor que poder dibujar el n칰mero en la pantalla y que este nos diga cual es, para este experimento lo primero que debemos hacer es guardar el modelo que creamos.

    model.save('modelo_mnist_keras')
**Esto crear치 una carpeta en el directorio donde estes trabajando.**

1. La app de tkinter, tiene la dependencia de PILLOW y numpy para procesar imagenes.
2. Creamos una interfaz para dibujar el n칰mero con el mouse y una vez dibujado, una funci칩n para procesar el digito, por ejemplo redimensionar y cambiarle el fondo para que se parezcan a los digitos mnist y que esten en el formato adecuado.
3. Creamos la funci칩n de predicci칩n que nos dir치 que n칰mero puede ser el que se ha dibujado.
 
Puedes ver el c칩digo completo [aqui](tkinter_prediction_app.py)

## Algunas dudas que me surgieron e investigu칠 un poco.

- **Por que en los libros veia 64, 128 y hasta 1024 unidades en la capa oculta Dense (totalmente conectada)?**

R: En general, una mayor cantidad de unidades en las capas densas permite que el modelo **aprenda representaciones m치s complejas** y no tan b치sicas de los datos de entrada. Sin embargo, agregar m치s unidades tambi칠n aumenta la cantidad de par치metros en el modelo, lo que puede lleva a un **mayor consumo de recursos** y un mayor **riesgo de sobreajuste.**

En este caso, utilic칠 varios, pero con el valor de 128 unidades obtuve un rendimiento adecuado. Este valor es com칰nmente utilizado en una variedad de tareas de clasificaci칩n de im치genes, incluso lo he visto en muchas implementaciones de reconocimiento de d칤gitos MNIST.

_El n칰mero 칩ptimo de unidades en las capas densas puede variar dependiendo del problema espec칤fico y de la arquitectura del modelo._

- **Por qu칠 la funcion de activaci칩n ReLU para las capas totalmente conectadas del medio de la red?**

R: ReLU introduce _no linealidad_ en la red neuronal, lo que **permite al modelo aprender relaciones complejas** entre las caracter칤sticas de entrada y las salidas. Las redes neuronales sin funciones de activaci칩n no lineales como ReLU ser칤an equivalentes a modelos lineales, lo que conlleva que no aprendan patrones dificiles en los datos sino relaciones lineales simples.

- **쮺칩mo funciona ReLU?**

Cuando se aplica la funci칩n ReLU a una neurona, si la entrada es mayor que cero, la salida ser치 igual a la entrada. Si la entrada es menor o igual a cero, la salida ser치 cero. En t칠rminos gr치ficos, la funci칩n ReLU traza una l칤nea recta que comienza en el origen y se extiende hacia arriba en un 치ngulo de 45 grados.

<img src="media/relu.jpg" alt="ReLUT" style="width:400px;"> 

[M치s sobre ReLU](https://es.wikipedia.org/wiki/Rectificador_(redes_neuronales))


Espero que esta breve explicaci칩n ayude a comprender un poco mejor algunos conceptos de el impresionante 游Deep learning游, a mi me ha servido bastante para practicar y entender los principios de redes neuronales

### Conocimiento y agradecimientos:
- https://github.com/rasbt/python-machine-learning-book-3rd-edition
- https://twitter.com/santiagohramos

### Gracias por leer.九덢잺



