# Another-mnistPredictor-in-tensorflow-pero-puedes-usarlo

## Esta es, la implementacion de una red neuronal para predecir digitos escritos a mano. Pero esta vez, he creado una aplicacion de tkinter para poder escribir n√∫meros y que el modelo nos diga la prediccion.

La identificaci√≥n de n√∫meros escritos a mano mediante redes neuronales es una tarea clave en la visi√≥n por computadora. Utilizando conjuntos de datos etiquetados, **como el conjunto de datos MNIST**, que contiene miles de im√°genes de n√∫meros y sus etiquetas reales. Estos modelos aprenden a reconocer y clasificar d√≠gitos num√©ricos lo cual tiene aplicaciones en procesamiento de im√°genes, digitalizaci√≥n de documentos, clasificaci√≥n autom√°tica de formularios y detecci√≥n de fraudes.

**Como he mencionado, vamos a entrenar un modelo de red neuronal utilizando el conjunto de datos [MNIST](https://datascience.eu/es/procesamiento-del-lenguaje-natural/base-de-datos-del-mnist/#:~:text=la%20base%20de%20datos%20del,sistemas%20de%20manejo%20de%20im%C3%A1genes.)**
<img src="media/mnist2.png" alt="Numero 5 de MNIST" style="width:700px;">


### üë®‚Äçüè´ Voy a intentar explicar cada uno de los pasos de manera sencilla y tambien puedes ver el c√≥digo en el Jupyter notebook de este repositorio.

### Paso 1: Importar librerias y Cargar el dataset MNIST

Se carga el dataset MNIST que contiene las im√°genes de entrenamiento y prueba, as√≠ como las etiquetas correspondientes que indican qu√© d√≠gito representa cada imagen.

    import tensorflow as tf
    from tensorflow.keras.datasets import mnist
    import numpy as np
  
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

### Paso 2: Preprocesamiento de los datos

     x_train = x_train / 255.0 
     x_test = x_test / 255.0

Las imagenes de MNIST est√°n en escala de grises. Primero se realiza un preprocesamiento de los datos dividiendo los valores de cada pixel entre 255 para normalizarlos y que est√©n en un rango de 0 a 1. La escala de grises generalmente se representa en n√∫meros de 0 a 255.

**Esto facilita el paso por la red neuronal.**

> En este rango, el valor 0 representa el **negro absoluto (sin intensidad de luz)** y el valor 255 representa el **blanco absoluto** (m√°xima intensidad de luz).
>> La raz√≥n por la cual se utiliza el rango de 0 a 255 es por la representaci√≥n de 8 bits, donde cada p√≠xel en una imagen _en blanco y negro_, se almacena como un valor de 8 bits (1 byte). Con 8 bits, se pueden representar 2^8 = 256 valores distintos, es decir, desde 0 hasta 255. Cada valor representa un nivel de intensidad de luz en la escala de grises.

### Paso 3: Crear el modelo de la red neuronal

    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Flatten, Dense
    
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28)))
    model.add(Dense(units=128, activation='relu'))
    model.add(Dense(units=128, activation='relu'))
    model.add(Dense(units=128, activation='relu'))
    model.add(Dense(units=10, activation='softmax'))
    
1. Creamos el modelo de red neuronal utilizando la API de Keras. El modelo consta de una capa **Flatten()** o de _aplanamiento_ que convierte las caracter√≠sticas 2D en un vector 1D. 
2. Seguimos con tres capas ocultas Dense(units=128, activation='relu'): las capas Dense son capas completamente conectadas con **128 unidades (neuronas)** y funci√≥n de activaci√≥n ReLU(_explicaci√≥n m√°s abajo_). Debo decir que esto puede variar y podriamos probar por ejemplo con una sola capa totalmente conectada pero de 1024 unidades.
3. Por ultimo una capa de salida **fully connected** (Dense) pero con 10 unidades (una para cada d√≠gito posible) y funci√≥n de activaci√≥n softmax. La funci√≥n softmax asigna probabilidades a cada clase por eso se utiliza en problemas de clasificaci√≥n multiclase.

### Paso 4: Compilar el modelo.

    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
              
B√°sicamente, compilar es definir el modelo y sus hiperpar√°metros, especificando el optimizador, la funci√≥n de p√©rdida y las m√©tricas que se utilizar√°n durante el entrenamiento. En este caso, se utiliza el optimizador [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), la funci√≥n de p√©rdida **sparse_categorical_crossentropy** es apropiada para clasificaci√≥n multiclase adem√°s medimos la precisi√≥n.
![Funcion de perdida](media/loss.png)

### Paso 5: Entrenar el modelo.
Se entrena el modelo utilizando los datos de entrenamiento durante 50 epochs. Durante el entrenamiento, el modelo aprender√° a reconocer y clasificar los d√≠gitos de el dataset MNIST

### Paso 6: Evaluar el modelo con el conjunto de prueba.

![Grafica de perdida y precision](media/grafica.png)

Una vez entrenado el modelo, se eval√∫a su rendimiento utilizando el conjunto de prueba. Se calcula la p√©rdida y la precisi√≥n mientras ve√≠a las im√°genes de prueba. La precisi√≥n indica qu√© tan bien clasifica el modelo los d√≠gitos que nunca ha visto.

## Algunas dudas que me surgieron e investigu√© un poco.

- **Por que en los libros veia 64, 128 y hasta 1024 unidades en la capa oculta Dense (totalmente conectada)?**

R: En general, una mayor cantidad de unidades en las capas densas permite que el modelo **aprenda representaciones m√°s complejas** y no tan b√°sicas de los datos de entrada. Sin embargo, agregar m√°s unidades tambi√©n aumenta la cantidad de par√°metros en el modelo, lo que puede lleva a un **mayor consumo de recursos** y un mayor **riesgo de sobreajuste.**

En este caso, utilic√© varios, pero con el valor de 128 unidades obtuve un rendimiento adecuado. Este valor es com√∫nmente utilizado en una variedad de tareas de clasificaci√≥n de im√°genes, incluso lo he visto en muchas implementaciones de reconocimiento de d√≠gitos MNIST.

_El n√∫mero √≥ptimo de unidades en las capas densas puede variar dependiendo del problema espec√≠fico y de la arquitectura del modelo._

- **Por qu√© la funcion de activaci√≥n ReLU para las capas totalmente conectadas del medio de la red?**

R: ReLU introduce _no linealidad_ en la red neuronal, lo que **permite al modelo aprender relaciones complejas** entre las caracter√≠sticas de entrada y las salidas. Las redes neuronales sin funciones de activaci√≥n no lineales como ReLU ser√≠an equivalentes a modelos lineales, lo que conlleva que no aprendan patrones dificiles en los datos sino relaciones lineales simples.

- **¬øC√≥mo funciona ReLU?**

Cuando se aplica la funci√≥n ReLU a una neurona, si la entrada es mayor que cero, la salida ser√° igual a la entrada. Si la entrada es menor o igual a cero, la salida ser√° cero. En t√©rminos gr√°ficos, la funci√≥n ReLU traza una l√≠nea recta que comienza en el origen y se extiende hacia arriba en un √°ngulo de 45 grados.

<img src="media/relu.jpg" alt="ReLUT" style="width:400px;"> 

[M√°s sobre ReLU](https://es.wikipedia.org/wiki/Rectificador_(redes_neuronales))

**Espero que esta breve explicaci√≥n ayude a comprender un poco mejor algunos conceptos de el maravilloso üå†Deep learningüå†, a mi me ha servido bastante para practicar y entender algunos principios de redes neuronales**

### Gracias por leer.‚úîÔ∏è


